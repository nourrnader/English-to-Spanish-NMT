# English-to-Spanish-NMT
This project explores machine translation from English to Spanish using three different sequence-to-sequence (seq2seq) models: an LSTM-based model, a Transformer model, and a pre-trained Transformer model from the Helsinki-NLP repository. The goal was to compare their performance in translating text while addressing challenges like long-range dependencies and computational efficiency.
# Results

    Pre-trained Transformer: Achieved near-perfect translations, leveraging prior training on large multilingual datasets.

    Transformer: Achieved 83% accuracy, excelling at capturing long-range dependencies.

    LSTM: Least efficient, struggling with long sentences and slower training due to sequential processing
