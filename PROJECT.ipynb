{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Final Project - Machine Translation**"
      ],
      "metadata": {
        "id": "v-CTrwRC_oDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Downloading Libraries"
      ],
      "metadata": {
        "id": "WjKctk1Fzn-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDKI8xSNg4a-",
        "outputId": "0c523a93-a119-4787-e2e8-e5864ce86a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow-text)\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow-text)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text) (0.1.2)\n",
            "Downloading tensorflow_text-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-text-2.18.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOavaibPgMso",
        "outputId": "8b194b16-d50d-40c3-f28e-5b426b01f967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "QRh7tUtBU2oV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8eXxaCCfx7v",
        "outputId": "40e9a885-44f9-4d9e-b0b5-4f7c8a0a19e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  english  spanish\n",
            "0     Go.      Ve.\n",
            "1     Go.    Vete.\n",
            "2     Go.    Vaya.\n",
            "3     Go.  Váyase.\n",
            "4     Hi.    Hola.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, LayerNormalization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/colab/data.csv')\n",
        "print(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CkDb0fyOfx7x",
        "outputId": "8e142380-ecca-409a-ee85-6e9f03a24eca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      english  \\\n",
              "0      Everyone has strengths and weaknesses.   \n",
              "1           I'm glad that he passed the exam.   \n",
              "2             I wish I had a house of my own.   \n",
              "3  We are committed to our country's welfare.   \n",
              "4         They say that he will never return.   \n",
              "\n",
              "                                            spanish  \n",
              "0     Todo el mundo tiene fortalezas y debilidades.  \n",
              "1           Me alegro de que haya pasado el examen.  \n",
              "2                   Desearía tener una casa propia.  \n",
              "3  Estamos comprometidos con el bienestar del país.  \n",
              "4                  Se dice que él ya nunca volverá.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e9c9252-a72c-494d-8b58-1ba6b3876467\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Everyone has strengths and weaknesses.</td>\n",
              "      <td>Todo el mundo tiene fortalezas y debilidades.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm glad that he passed the exam.</td>\n",
              "      <td>Me alegro de que haya pasado el examen.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I wish I had a house of my own.</td>\n",
              "      <td>Desearía tener una casa propia.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We are committed to our country's welfare.</td>\n",
              "      <td>Estamos comprometidos con el bienestar del país.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They say that he will never return.</td>\n",
              "      <td>Se dice que él ya nunca volverá.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e9c9252-a72c-494d-8b58-1ba6b3876467')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e9c9252-a72c-494d-8b58-1ba6b3876467 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e9c9252-a72c-494d-8b58-1ba6b3876467');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23fa9331-e4b5-42e0-9390-13c97b8dce9c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23fa9331-e4b5-42e0-9390-13c97b8dce9c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23fa9331-e4b5-42e0-9390-13c97b8dce9c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Shuffle dataset\n",
        "\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl__U4Mxfx70",
        "outputId": "f8e2abb6-886a-4689-8781-6eaccbe6d9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "en_text = data[\"english\"]\n",
        "spn_text = data[\"spanish\"]\n",
        "import string\n",
        "en_text=en_text.to_numpy().tolist()\n",
        "en_text = [str(item) for item in en_text]\n",
        "spn_text=spn_text.to_numpy().tolist()\n",
        "spn_text = [str(item) for item in spn_text]\n",
        "print(type(en_text))  # Should be <class 'list'>\n",
        "print(type(en_text[0]))  # Should be <class 'str'>, not a tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "def pre_proc(text):\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode('utf-8')  # Decode bytes to string using 'utf-8' encoding\n",
        "\n",
        "    text = unicodedata.normalize('NFKD', text)  # removes accents in Spanish\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'[^ a-z.?!,¿]', '', text)  # removes any characters that are not letters/numbers/ .?!\n",
        "    text = re.sub(r'[.?!,¿]', r' \\0 ', text)  # adds space before and after punctuations\n",
        "    text = re.sub(r'\\bstart\\b', '', text)  # Remove \"start\"\n",
        "    text = re.sub(r'\\bend\\b', '', text)  # Remove \"end\"\n",
        "    text = f'[START] {text} [END]'  # Add [START] and [END] tags\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "opfn_BVQS3ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_train=en_text[:10000]\n",
        "spn_train=spn_text[:10000]\n",
        "en_test=en_text[10000:12000]\n",
        "spn_test=spn_text[10000:12000]"
      ],
      "metadata": {
        "id": "wabY1bkD_W-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_train = [pre_proc(text) for text in en_train]\n",
        "spn_train = [pre_proc(text) for text in spn_train]\n",
        "en_test = [pre_proc(text) for text in en_test]\n",
        "spn_test = [pre_proc(text) for text in spn_test]\n",
        "print(en_test[:7])\n",
        "print(spn_test[:7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaLqii2qRmB4",
        "outputId": "d7abf273-55a9-479c-bca6-c1503b42d7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[START]   hes very intelligent     [END]', '[START]   the bus is capable of carrying thirty people     [END]', '[START]   when did you lose your keys     [END]', '[START]   im not interested in anything tom has to say     [END]', '[START]   she tried to hide her feelings     [END]', '[START]   tom knows who mary is     [END]', '[START]   the same thing happened monday     [END]']\n",
            "['[START]   el es muy inteligente     [END]', '[START]   este autobus tiene capacidad para treinta personas     [END]', '[START]     cuando perdiste las llaves     [END]', '[START]   no me interesa nada de lo que tom tenga para decir     [END]', '[START]   intento ocultar sus sentimientos     [END]', '[START]   tom sabe quien es mary     [END]', '[START]   lo mismo paso el lunes     [END]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dja-MmqHQyA5",
        "outputId": "fb6cdb66-f854-4008-ccd9-c4dd92c7cb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words in the English vocabulary: 4991\n",
            "The number of words in the Spanish vocabulary: 7479\n"
          ]
        }
      ],
      "source": [
        "num_words = 10000\n",
        "tokenizer_en= Tokenizer(num_words=num_words, filters='#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer_spn= Tokenizer(num_words=num_words, filters='#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer_en.fit_on_texts(en_train)\n",
        "tokenizer_spn.fit_on_texts(spn_train)\n",
        "en_train = tokenizer_en.texts_to_sequences(en_train)\n",
        "spn_train = tokenizer_spn.texts_to_sequences(spn_train)\n",
        "\n",
        "word_idx_en = tokenizer_en.word_index\n",
        "word_idx_spn = tokenizer_spn.word_index\n",
        "print(f\"The number of words in the English vocabulary: {len(word_idx_en)}\")\n",
        "print(f\"The number of words in the Spanish vocabulary: {len(word_idx_spn)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TudG9LoZddI"
      },
      "outputs": [],
      "source": [
        "en_train = pad_sequences(en_train, maxlen = 30, padding='post', truncating='post')\n",
        "spn_train = pad_sequences(spn_train, maxlen=30, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl5qWnyZZlIS",
        "outputId": "206ec9f2-3743-4007-a2fc-d1f15d96d6a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2,  335,   46, 2675,   40, 2676,    1,    3,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,   35,  492,   14,   11,  700,    4,  835,    1,    3,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,    5,  336,    5,   50,    9,  119,   13,   17,  301,    1,\n",
              "           3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,   31,   23, 1918,    6,  114, 2677, 2678,    1,    3,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,   45,  133,   14,   11,   49,   97,  701,    1,    3,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "en_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization and padding to the test data\n",
        "en_test = tokenizer_en.texts_to_sequences(en_test)\n",
        "spn_test = tokenizer_spn.texts_to_sequences(spn_test)\n",
        "en_test = pad_sequences(en_test, maxlen = 30, padding='post', truncating='post')\n",
        "spn_test = pad_sequences(spn_test, maxlen=30, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "kpcAI145SErl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM-based Seq2Seq Model** (Bonus)"
      ],
      "metadata": {
        "id": "FHfTgzIRQbwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras import optimizers"
      ],
      "metadata": {
        "id": "F-IYAf6vrjon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_en = len(tokenizer_en.word_index) + 1\n",
        "vocab_spn = len(tokenizer_spn.word_index) + 1"
      ],
      "metadata": {
        "id": "1nqN3r_CSTDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_train= en_train[:10000]\n",
        "spn_train= spn_train[:10000]\n",
        "en_val= en_train[10000:12000]\n",
        "spn_val= spn_train[10000:12000]\n",
        "print(en_train.shape)\n",
        "print(spn_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czk-YOqe0uld",
        "outputId": "edcdd1cd-ebac-4f74-90dd-320f656d8071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_en, output_dim=128,input_shape=(en_len,), mask_zero=True))\n",
        "model.add(LSTM(units=512)) # Encoder layer\n",
        "model.add(RepeatVector(n=spn_len))\n",
        "model.add(LSTM(units=512, return_sequences=True)) # Decoder layer\n",
        "model.add(Dense(vocab_spn, activation='softmax')) # Generates the probabilities for each word in the target vocabulary (Spanish)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSUXhzIzxJzw",
        "outputId": "c36bf726-3e7f-45fe-be5f-ef703e26817f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "MXZ7C1n2xgnt",
        "outputId": "c9737a59-d428-4edf-85ed-0092c209cd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m696,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,312,768\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8243\u001b[0m)           │       \u001b[38;5;34m4,228,659\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">696,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8243</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,228,659</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,337,331\u001b[0m (31.80 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,337,331</span> (31.80 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,337,331\u001b[0m (31.80 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,337,331</span> (31.80 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocabulary size for Spanish: {vocab_spn}\")\n",
        "print(f\"Vocabulary size for English: {vocab_en}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0akxumG1nvk",
        "outputId": "8c27cd98-94b0-4470-bdf2-5c5c45874dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size for Spanish: 8243\n",
            "Vocabulary size for English: 5443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Evaluation"
      ],
      "metadata": {
        "id": "gsPlRBYMNWcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "# Define optimizer\n",
        "rms = RMSprop(learning_rate=0.001)\n",
        "# Compile the model\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the model\n",
        "# Limit the target values to be within the vocabulary size\n",
        "#spn_train = np.clip(spn_train, 0, vocab_spn - 1)\n",
        "\n",
        "history = model.fit(en_train, spn_train,validation_data=(en_val, spn_val), epochs=10, batch_size=32)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-24T00:58:22.187302Z",
          "iopub.execute_input": "2024-12-24T00:58:22.187557Z",
          "iopub.status.idle": "2024-12-24T01:02:44.075242Z",
          "shell.execute_reply.started": "2024-12-24T00:58:22.187536Z",
          "shell.execute_reply": "2024-12-24T01:02:44.074481Z"
        },
        "id": "AEdCSXvKNDF_",
        "outputId": "c84e03b9-e29c-45e3-9c39-73f73ee1911e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 71ms/step - accuracy: 0.8933 - loss: 1.3206 - val_accuracy: 0.9224 - val_loss: 0.4827\nEpoch 2/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 73ms/step - accuracy: 0.9230 - loss: 0.4822 - val_accuracy: 0.9224 - val_loss: 0.4744\nEpoch 3/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 76ms/step - accuracy: 0.9249 - loss: 0.4631 - val_accuracy: 0.9242 - val_loss: 0.4654\nEpoch 4/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 80ms/step - accuracy: 0.9260 - loss: 0.4543 - val_accuracy: 0.9230 - val_loss: 0.4706\nEpoch 5/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 86ms/step - accuracy: 0.9257 - loss: 0.4536 - val_accuracy: 0.9269 - val_loss: 0.4564\nEpoch 6/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step - accuracy: 0.9264 - loss: 0.4498 - val_accuracy: 0.9256 - val_loss: 0.4599\nEpoch 7/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 86ms/step - accuracy: 0.9266 - loss: 0.4462 - val_accuracy: 0.9256 - val_loss: 0.4555\nEpoch 8/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 87ms/step - accuracy: 0.9269 - loss: 0.4449 - val_accuracy: 0.9257 - val_loss: 0.4599\nEpoch 9/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step - accuracy: 0.9273 - loss: 0.4413 - val_accuracy: 0.9248 - val_loss: 0.4654\nEpoch 10/10\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 88ms/step - accuracy: 0.9279 - loss: 0.4393 - val_accuracy: 0.9274 - val_loss: 0.4526\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "test_accuracy = model.evaluate(en_test, spn_test)[1]\n",
        "print(f\"Model's Accuracy: {test_accuracy*100}%\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-24T09:20:27.122767Z",
          "iopub.execute_input": "2024-12-24T09:20:27.123065Z",
          "iopub.status.idle": "2024-12-24T09:20:29.531349Z",
          "shell.execute_reply.started": "2024-12-24T09:20:27.123042Z",
          "shell.execute_reply": "2024-12-24T09:20:29.530670Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63fa97c6-70b1-489a-ff0a-efc70b385ed8",
        "id": "yIDG3eoz_KNR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Accuracy: 78.67%\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "FiifEW24GSAG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNSGFSOmZtwW"
      },
      "source": [
        "## Position Encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSJrUOV8Zu3M"
      },
      "outputs": [],
      "source": [
        "# Function to compute the angles for positional encoding.\n",
        "def get_angles(pos, i, emb_dim):\n",
        "    \"\"\"\n",
        "    pos: The position of the token in the sequence (e.g., 0, 1, 2, ..., seq_len-1).\n",
        "    i: The index of the embedding dimension (e.g., 0, 1, ..., embedding_dim-1).\n",
        "    embedding_dim: The total dimensionality of the embedding space (e.g., 512, 256, etc.).\n",
        "    \"\"\"\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(emb_dim))\n",
        "    return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW6kZ6LGaXy-"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, embedding_dim):\n",
        "    \"\"\"\n",
        "    Adds  positional encoding to the Embeddings to be fed to the Transformer model.\n",
        "\n",
        "    Computes a sin and cos of the angles determined by the get_angles() function\n",
        "    and adds the value computed to an axis of the embeddings.\n",
        "    \"\"\"\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                           np.arange(embedding_dim)[np.newaxis, :], embedding_dim)\n",
        "\n",
        "    # apply sin to even indices in the array. ie 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array. ie 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate positional encodings\n",
        "pos_encodings = positional_encoding(100, 128)"
      ],
      "metadata": {
        "id": "bbbULKUt1EYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ytpwB7Lp91"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GcFqsolLtlL"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    Creates a padding mask for a given sequence.\n",
        "\n",
        "    Args:\n",
        "        seq (tensor): A tensor of shape (batch_size, seq_len) containing the sequence.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of shape (batch_size, 1, 1, seq_len) containing a mask that is 1 where the sequence is padded, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    # Convert the sequence to a boolean tensor where True indicates a pad token (value 0).\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # Add an extra dimension to the mask to add the padding to the attention logits (ensure mask can be applied in attention during self-attention).\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### This mask ensures that, during training, the model cannot \"see\" future tokens in the sequence, and it can only attend to the current or previous tokens. This is necessary because the model should predict each token in a sequence autoregressively, one token at a time."
      ],
      "metadata": {
        "id": "4uRr1V1nXh3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_GEkYgwL1cv"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Creates a look-ahead mask used during training the decoder of a transformer.\n",
        "\n",
        "    Args:\n",
        "        size (int): The size of the mask.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: A lower triangular matrix of shape (size, size) with ones on the diagonal\n",
        "            and zeros below the diagonal. (indicating \"allowed\" positions)\n",
        "    \"\"\"\n",
        "    # create a matrix with ones on the diagonal and zeros below the diagonal\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "594TrwPPL7pE"
      },
      "outputs": [],
      "source": [
        "# Responsible for generating all the necessary masks used in the transformer model\n",
        "def create_masks(inputs, targets):\n",
        "    \"\"\"\n",
        "    Creates masks for the input sequence and target sequence.\n",
        "\n",
        "    Args:\n",
        "        inputs: Input sequence tensor.\n",
        "        targets: Target sequence tensor.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of three masks: the encoder padding mask, the combined mask used in the first attention block,\n",
        "        and the decoder padding mask used in the second attention block.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the encoder padding mask.\n",
        "    enc_padding_mask = create_padding_mask(inputs)\n",
        "\n",
        "    # Create the decoder padding mask.\n",
        "    dec_padding_mask = create_padding_mask(inputs)\n",
        "\n",
        "    # Create the look ahead mask for the first attention block.\n",
        "    # It is used to pad and mask future tokens in the tokens received by the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
        "\n",
        "    # Create the decoder target padding mask.\n",
        "    dec_target_padding_mask = create_padding_mask(targets)\n",
        "\n",
        "    # Combine the look ahead mask and decoder target padding mask for the first attention block.\n",
        "    # Ensures that the decoder’s self-attention mechanism does not attend to padding tokens or future tokens.\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "5WgutrYfaNGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Attention"
      ],
      "metadata": {
        "id": "nT2iHaEpaOX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Computes the scaled dot product attention weight for the query (q), key (k), and value (v) vectors.\n",
        "    The attention weight is a measure of how much focus should be given to each element in the sequence of values (v)\n",
        "    based on the corresponding element in the sequence of queries (q) and keys (k).\n",
        "\n",
        "    Args:\n",
        "    q: query vectors; shape (..., seq_len_q, depth)\n",
        "    k: key vectors; shape  (..., seq_len_k, depth)\n",
        "    v: value vectors; shape  (..., seq_len_v, depth_v)\n",
        "    mask: (optional) mask to be applied to the attention weights\n",
        "\n",
        "    Returns:\n",
        "    output: The output of the scaled dot product attention computation; shape   (..., seq_len_q, depth_v)\n",
        "    attention_weights: The attention weights\n",
        "    \"\"\"\n",
        "    # Compute dot product of query and key vectors\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # Compute the square root of the depth of the key vectors\n",
        "    dk = tf.cast(tf.shape(k)[-1], dtype=tf.float32)\n",
        "    scaled_dk = tf.math.sqrt(dk)\n",
        "\n",
        "    # Compute scaled attention logits by dividing dot product by scaled dk\n",
        "    # To prevent excessively large values in the attention logits\n",
        "    scaled_attention_logits = matmul_qk / scaled_dk\n",
        "\n",
        "    # Apply mask to the attention logits (if mask is available)\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  #By adding -1e9 where the mask is 1, attention weight for these positions becomes effectively 0 after applying softmax\n",
        "\n",
        "    # Apply softmax to the scaled attention logits to get the attention weights\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # Compute the weighted sum of the value vectors using the attention weights\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "gvnsczULaTVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head Attention"
      ],
      "metadata": {
        "id": "Cqn8xghjgnZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    MultiHeadAttention Layer that implements the attention mechanism for the Transformer.\n",
        "    It splits the input into multiple heads, computes scaled dot-product attention for each head\n",
        "    and then concatenates the output of the heads and passes it through a dense layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, key_dim, num_heads, dropout_rate=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the MultiHeadAttention layer.\n",
        "\n",
        "        Args:\n",
        "            key_dim (int): The dimensionality of the key space.\n",
        "            num_heads (int): The number of attention heads.\n",
        "            dropout (float): The dropout rate to apply after the dense layer.\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        # ensure  that the dimension of the embedding can be evenly split across attention heads\n",
        "        assert key_dim % num_heads == 0\n",
        "        self.depth = self.key_dim // self.num_heads  # dimension of each individual head\n",
        "\n",
        "        # dense layers to project the input into queries, keys and values\n",
        "        self.wq = Dense(key_dim)\n",
        "        self.wk = Dense(key_dim)\n",
        "        self.wv = Dense(key_dim)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "        # dense layer to project the output of the attention heads\n",
        "        self.dense = Dense(key_dim)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Splits the last dimension of the tensor into (num_heads, depth).\n",
        "        Transposes the result such that the shape is (batch_size, num_heads, seq_len, depth).\n",
        "\n",
        "        Args:\n",
        "            x (tensor): The tensor to be split.\n",
        "            batch_size (int): The size of the batch.\n",
        "\n",
        "        Returns:\n",
        "            tensor: The tensor with the last dimension split into (num_heads, depth) and transposed.\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        \"\"\"\n",
        "        Applies the multi-head attention mechanism to the inputs.\n",
        "\n",
        "        Args:\n",
        "            v (tensor): The value tensor of shape (batch_size, seq_len_v, key_dim).\n",
        "            k (tensor): The key tensor of shape (batch_size, seq_len_k, key_dim).\n",
        "            q (tensor): The query tensor of shape (batch_size, seq_len_q, key_dim).\n",
        "            mask (tensor, optional): The mask tensor of shape (batch_size, seq_len_q, seq_len_k).\n",
        "                                     Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tensor: The output tensor of shape (batch_size, seq_len_q, key_dim).\n",
        "            tensor: The attention weights tensor of shape (batch_size, num_heads, seq_len_q, seq_len_k).\n",
        "        \"\"\"\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # input tensors are passed through dense layers to project them into correct key dim\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # split the heads\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # split the queries, keys and values into multiple heads (compute the attention output and the attention weights)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # reshape and add Dense layer\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
        "        output = self.dense(concat_attention)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "JNdRL5bngyaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FC Layer"
      ],
      "metadata": {
        "id": "KfydYCumjxgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FeedForward(embedding_dim, fully_connected_dim):\n",
        "    \"\"\"Create a fully connected feedforward neural network.\n",
        "\n",
        "    Args:\n",
        "        embedding_dim (int): Dimensionality of the embedding output from the transformer layer.\n",
        "        fully_connected_dim (int): Number of neurons in the fully connected layers.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Sequential: A fully connected feedforward neural network with the specified architecture.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),\n",
        "        tf.keras.layers.Dense(embedding_dim)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "tFIEbBKRj0ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "Klz7u5xR-8yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######  Encoder attends to all positions of the input sequence to compute a weighted sum of the values at each position which  allows to capture dependencies between all positions. Each layer in Encoder has residual connections and layer normalization, which help to mitigate the vanishing gradient problem and improve training stability."
      ],
      "metadata": {
        "id": "MamAAh6g--xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1):\n",
        "        \"\"\"Initializes the encoder layer\n",
        "\n",
        "        Args:\n",
        "            embedding_dim: The dimensionality of the input and output of this layer\n",
        "            num_heads: The number of attention heads to use in the multi-head attention layer\n",
        "            fully_connected_dim: The dimensionality of the hidden layer in the feedforward network\n",
        "            dropout_rate: The rate of dropout to apply to the output of this layer during training\n",
        "\n",
        "        Returns:\n",
        "            A new instance of the EncoderLayer class\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.multi = MultiHeadAttention(embedding_dim, num_heads, dropout_rate)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.fc = FeedForward(embedding_dim, fully_connected_dim)\n",
        "\n",
        "    def call(self, x, mask, training):\n",
        "        \"\"\"Applies the encoder layer to the input tensor\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor to the encoder layer\n",
        "            training: A boolean indicating whether the model is in training mode\n",
        "            mask: A tensor representing the mask to apply to the attention mechanism\n",
        "\n",
        "        Returns:\n",
        "            The output of the encoder layer after applying the multi-head attention and feedforward network\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply multi-head self-attention mechanism to input tensor\n",
        "        attn_out, _ = self.multi(x, x, x, mask)\n",
        "\n",
        "        # Apply first layer normalization and add residual connection\n",
        "        out1 = self.layernorm1(attn_out + x)\n",
        "\n",
        "        # Apply feedforward network to output of first layer normalization\n",
        "        fc_out = self.fc(out1)\n",
        "        fc_out = self.dropout(fc_out, training=training)\n",
        "\n",
        "        # Apply second layer normalization and add residual connection\n",
        "        out2 = self.layernorm2(fc_out + out1)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "UZyKXk36_x_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, emb_dim, num_heads, fc_dim,vocab_size, max_position, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder layer of the Transformer model.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): Number of EncoderLayers to stack.\n",
        "            emb_dim (int): Dimensionality of the token embedding space.\n",
        "            num_heads (int): Number of attention heads to use in MultiHeadAttention layers.\n",
        "            fc_dim (int): Dimensionality of the fully connected layer in the EncoderLayer.\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_position (int): Maximum length of input sequences for positional encoding.\n",
        "            dropout_rate (float): Probability of dropping out units during training.\n",
        "\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = positional_encoding(max_position, emb_dim)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.enc_layers = [EncoderLayer(emb_dim, num_heads, fc_dim, dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        \"\"\"\n",
        "        Call function for the Encoder layer.\n",
        "\n",
        "        Args:\n",
        "            inputs: tensor of shape (batch_size, sequence_length) representing input sequences\n",
        "            training: boolean indicating if the model is in training mode\n",
        "            mask: tensor of shape (batch_size, sequence_length) representing the mask to apply to the input sequence\n",
        "\n",
        "        Returns:\n",
        "            A tensor of shape (batch_size, sequence_length, embedding_dim) representing the encoded sequence\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the sequence length\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Embed the input sequence\n",
        "        inputs = self.embedding(inputs)\n",
        "\n",
        "        # Scale the embeddings by sqrt(embedding_dim)\n",
        "        inputs *= tf.math.sqrt(tf.cast(self.emb_dim, tf.float32))\n",
        "\n",
        "        # Add positional encodings to the input sequence\n",
        "        inputs += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # Apply dropout to the input sequence\n",
        "        inputs = self.dropout(inputs, training=training)\n",
        "\n",
        "        # Pass the input sequence through the encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            inputs = self.enc_layers[i](inputs, mask, training=training)\n",
        "\n",
        "        # Return the encoded sequence\n",
        "        return inputs"
      ],
      "metadata": {
        "id": "VAUtZevLD-P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "VSLgUalyFjZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### The decoder takes in the encoded input sequence along with previous generated output sequence. The output sequence is first passed through an embedding layer, which maps each token to a high-dimensional vector space. The embedding output is then added with a positional encoding, which allows the model to encode the sequential order of the input/output sequence."
      ],
      "metadata": {
        "id": "0W1oe1H4FlQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Decoder applies a multi-head self-attention mechanism similar to encoder. However, decoder also uses an additional masked self-attention mechanism, which prevents it from attending to future tokens in output sequence during training."
      ],
      "metadata": {
        "id": "v544-wXyLOxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, emb_dim, num_heads, fc_dim, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes a single decoder layer of the transformer model.\n",
        "\n",
        "        Args:\n",
        "        emb_dim: The dimension of the embedding space.\n",
        "        num_heads: The number of attention heads to use.\n",
        "        fc_dim: The dimension of the feedforward network.\n",
        "        rate: The dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Instantiate two instances of MultiHeadAttention.\n",
        "        self.multi1 = MultiHeadAttention(emb_dim, num_heads, dropout_rate)\n",
        "        self.multi2 = MultiHeadAttention(emb_dim, num_heads, dropout_rate)\n",
        "\n",
        "        # Instantiate a fully connected feedforward network.\n",
        "        self.fc = FeedForward(emb_dim, fc_dim)\n",
        "\n",
        "        # Instantiate three layer normalization layers with epsilon=1e-6.\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Instantiate a dropout layer for regularization.\n",
        "        self.dropout3 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, pad_mask, training):\n",
        "        \"\"\"\n",
        "        Forward pass through the decoder layer.\n",
        "\n",
        "        Args:\n",
        "        x: The input to the decoder layer, a query vector.\n",
        "        enc_output: The output from the top layer of the encoder, a set of attention vectors k and v.\n",
        "        training: Whether to apply dropout regularization.\n",
        "        look_ahead_mask: The mask to apply to the input sequence so that it can't look ahead to future positions.\n",
        "        pad_mask: The mask to apply to the input sequence to ignore padding tokens.\n",
        "\n",
        "        Returns:\n",
        "        The output from the decoder layer, a tensor with the same shape as the input.\n",
        "        The attention weights from the first multi-head attention layer.\n",
        "        The attention weights from the second multi-head attention layer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply the first multi-head attention layer to the query vector x.\n",
        "        # We pass x as all three inputs to the layer because this is a self-attention layer.\n",
        "        attn1, attn_weights_block1 = self.multi1(x, x, x, look_ahead_mask)\n",
        "\n",
        "        # Add the original input to the output of the attention layer and apply layer normalization.\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        # Apply the second multi-head attention layer to the output from the first layer and the encoder output.\n",
        "        attn2, attn_weights_block2 = self.multi2(enc_output, enc_output, out1, pad_mask)\n",
        "\n",
        "        # Add the output from the first layer to the output of the second layer and apply layer normalization.\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        # Apply the feedforward network to the output of the second layer and apply dropout regularization.\n",
        "        fc_out = self.fc(out2)\n",
        "        fc_out = self.dropout3(fc_out, training=training)\n",
        "\n",
        "        # Add the output from the second layer to the output of the feedforward network and apply layer normalization.\n",
        "        out3 = self.layernorm3(fc_out + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "wu3qQPsLLdyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, emb_dim, num_heads, fc_dim, target_vocab_size, max_position, dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the Decoder object.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): The number of Decoder layers.\n",
        "            embedding_dim (int): The size of the embedding dimension.\n",
        "            num_heads (int): The number of heads in the MultiHeadAttention layer.\n",
        "            fully_connected_dim (int): The number of units in the feedforward network.\n",
        "            target_vocab_size (int): The number of words in the target vocabulary.\n",
        "            maximum_position_encoding (int): The maximum length of a sequence.\n",
        "            dropout_rate (float): The rate at which to apply dropout.\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        # create layers\n",
        "        self.embedding = Embedding(target_vocab_size, emb_dim)\n",
        "        self.pos_encoding = positional_encoding(max_position, emb_dim)\n",
        "        self.dec_layers = [DecoderLayer(emb_dim, num_heads, fc_dim, dropout_rate=0.1) for _ in range(num_layers)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, pad_mask, training):\n",
        "        \"\"\"\n",
        "        Executes the Decoder.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): The input to the Decoder.\n",
        "            enc_output (tf.Tensor): The output from the Encoder.\n",
        "            training (bool): Whether the Decoder is in training mode.\n",
        "            look_ahead_mask (tf.Tensor): The mask for self-attention in the MultiHeadAttention layer.\n",
        "            padding_mask (tf.Tensor): The mask for padding in the MultiHeadAttention layer.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: The output from the Decoder.\n",
        "            dict: A dictionary of attention weights.\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        # add embedding and positional encoding\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.emb_dim, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # apply each layer of the decoder\n",
        "        for i in range(self.num_layers):\n",
        "            # pass through decoder layer i\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, pad_mask, training=training)\n",
        "\n",
        "            # record attention weights for block1 and block2\n",
        "            attention_weights[f\"decoder_layer{i + 1}_block1\"] = block1\n",
        "            attention_weights[f\"decoder_layer{i + 1}_block2\"] = block2\n",
        "\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "mdBeR0STMfeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Class"
      ],
      "metadata": {
        "id": "wvrwB1O7NgUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Composed of two main components: the encoder and the decoder. The encoder takes an input sequence and produces a sequence of hidden representations, while the decoder takes this sequence of hidden representations and generates an output sequence"
      ],
      "metadata": {
        "id": "MtHPfsWnNoS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    A Transformer model that takes in an input and target sequence and outputs a final prediction.\n",
        "\n",
        "    Args:\n",
        "        num_layers (int): Number of layers in the Encoder and Decoder.\n",
        "        embedding_dim (int): Dimensionality of the embedding layer.\n",
        "        num_heads (int): Number of attention heads used in the Transformer.\n",
        "        fully_connected_dim (int): Dimensionality of the fully connected layer in the Encoder and Decoder.\n",
        "        input_vocab_size (int): Size of the input vocabulary.\n",
        "        target_vocab_size (int): Size of the target vocabulary.\n",
        "        max_positional_encoding_input (int): Maximum length of the input sequence.\n",
        "        max_positional_encoding_target (int): Maximum length of the target sequence.\n",
        "        dropout_rate (float, optional): Dropout rate used in the Encoder and Decoder layers. Defaults to 0.1.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, target_vocab_size, max_positional_encoding_input, max_positional_encoding_target, dropout_rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Initialize the Encoder and Decoder layers\n",
        "        self.encoder = Encoder(num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, max_positional_encoding_input, dropout_rate)\n",
        "        self.decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size, max_positional_encoding_target, dropout_rate)\n",
        "\n",
        "        # Add a final dense layer to make the final prediction\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask, training):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the Transformer model.\n",
        "\n",
        "        Args:\n",
        "            inp (tf.Tensor): Input sequence tensor with shape (batch_size, input_seq_len).\n",
        "            tar (tf.Tensor): Target sequence tensor with shape (batch_size, target_seq_len).\n",
        "            training (bool): Whether the model is being trained or not.\n",
        "            enc_padding_mask (tf.Tensor): Padding mask for the Encoder with shape (batch_size, 1, 1, input_seq_len).\n",
        "            look_ahead_mask (tf.Tensor): Mask to prevent the Decoder from looking ahead in the target sequence with shape (batch_size, 1, target_seq_len, target_seq_len).\n",
        "            dec_padding_mask (tf.Tensor): Padding mask for the Decoder with shape (batch_size, 1, 1, target_seq_len).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the final output of the model and the attention weights of the Decoder.\n",
        "        \"\"\"\n",
        "        # Pass the input sequence through the Encoder\n",
        "        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)\n",
        "\n",
        "        # Pass the target sequence and the output of the Encoder through the Decoder\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, look_ahead_mask=look_ahead_mask, pad_mask=dec_padding_mask, training=training)\n",
        "\n",
        "        # Pass the output of the Decoder through the final dense layer to get the final prediction\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "metadata": {
        "id": "i6O3okNrNiE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "vF8LQCV-34Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters for the Transformer model\n",
        "emb_dim = 256  # dimensionality of the embeddings used for tokens in the input and target sequences\n",
        "fully_connected_dim = 512  # dimensionality of the hidden layer of the feedforward neural network within the Transformer block\n",
        "num_layers = 4  # number of Transformer blocks in the encoder and decoder stacks\n",
        "num_heads = 8  # number of heads in the multi-head attention mechanism\n",
        "dropout_rate = 0.1  # dropout rate for regularization\n",
        "\n",
        "# Set vocabulary sizes for input and target sequences\n",
        "input_vocab_size = len(tokenizer_en.word_index) + 2  # add 2 for the start and end tokens\n",
        "target_vocab_size = len(tokenizer_spn.word_index) + 2  # add 2 for the start and end tokens\n",
        "\n",
        "# Set maximum positional encoding values for input and target sequences\n",
        "max_pos_encoding_input = input_vocab_size  # maximum positional encoding value for input sequence\n",
        "max_pos_encoding_target = target_vocab_size  # maximum positional encoding value for target sequence\n",
        "\n",
        "# Set the number of epochs and batch size for training\n",
        "EPOCHS = 10\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "nva3-KelI5O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"\n",
        "    A custom learning rate schedule that uses a combination of\n",
        "    a square root inverse decay and a warmup schedule.\n",
        "\n",
        "    Args:\n",
        "        embedding_dim (int): The dimension of the embedding.\n",
        "        warmup_steps (int): The number of steps used for warmup.\n",
        "\n",
        "    Returns:\n",
        "        float: The learning rate value at a given step.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_dim, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.emb_dim = tf.Variable(emb_dim, dtype=tf.float32)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\"\n",
        "        Compute the learning rate value for a given step using\n",
        "        a combination of square root inverse decay and warmup.\n",
        "\n",
        "        Args:\n",
        "            step (int): The current step number.\n",
        "\n",
        "        Returns:\n",
        "            float: The learning rate value at the current step.\n",
        "        \"\"\"\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.emb_dim) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "# Create an instance of the custom learning rate schedule\n",
        "learning_rate = CustomSchedule(emb_dim)"
      ],
      "metadata": {
        "id": "V4AyWGgxVqZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an instance of the Transformer model\n",
        "transformer = Transformer(num_layers, emb_dim, num_heads,\n",
        "                           fully_connected_dim, input_vocab_size, target_vocab_size,\n",
        "                           max_pos_encoding_input, max_pos_encoding_target, dropout_rate)\n",
        "\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2 = 0.98, epsilon = 1e-9)\n",
        "\n",
        "# Define the loss object\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "\n",
        "def loss_function(true_values, predictions):\n",
        "    \"\"\"\n",
        "    Calculate the loss value for a given target sequence.\n",
        "\n",
        "    Args:\n",
        "        true_values (tf.Tensor): The true target sequence.\n",
        "        predictions (tf.Tensor): The predicted target sequence.\n",
        "\n",
        "    Returns:\n",
        "        float: The loss value for the given target sequence.\n",
        "    \"\"\"\n",
        "    # Create a mask to exclude the padding tokens\n",
        "    mask = tf.math.logical_not(tf.math.equal(true_values, 0))\n",
        "\n",
        "    # Compute the loss value using the loss object\n",
        "    loss_ = loss_object(true_values, predictions)\n",
        "\n",
        "    # Apply the mask to exclude the padding tokens\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    # Calculate the mean loss value\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
        "\n",
        "def accuracy_function(true_values, predictions):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy for a given target sequence.\n",
        "\n",
        "    Args:\n",
        "        true_values (tf.Tensor): The true target sequence.\n",
        "        predictions (tf.Tensor): The predicted target sequence.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy value for the given target sequence.\n",
        "    \"\"\"\n",
        "    # Compute the accuracies using the true and predicted target sequences\n",
        "    accuracies = tf.equal(true_values, tf.argmax(predictions, axis=2))\n",
        "\n",
        "    # Create a mask to exclude the padding tokens\n",
        "    mask = tf.math.logical_not(tf.math.equal(true_values, 0))\n",
        "\n",
        "    # Apply the mask to exclude the padding tokens from the accuracies\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "    # Calculate the mean accuracy value\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n",
        "# Define the training metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "print(type(train_loss))  # Verify the class of train_loss\n",
        "print(type(train_accuracy))  #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLEObeunKXeS",
        "outputId": "000e808d-f597-460d-cdd2-d0cabeae357c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'keras.src.metrics.reduction_metrics.Mean'>\n",
            "<class 'keras.src.metrics.accuracy_metrics.SparseCategoricalAccuracy'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input signature for the train_step function\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # encoder_input\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64)   # target\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(encoder_input, target):\n",
        "    \"\"\"\n",
        "    Function to perform a single training step.\n",
        "\n",
        "    Args:\n",
        "    encoder_input (tf.Tensor): The input tensor for the encoder.\n",
        "    target (tf.Tensor): The target tensor for the decoder.\n",
        "\n",
        "    Returns:\n",
        "    None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Slice the target tensor to get the input for the decoder\n",
        "    decoder_input = target[:, :-1]\n",
        "\n",
        "    # Slice the target tensor to get the expected output of the decoder\n",
        "    expected_output = target[:, 1:]\n",
        "\n",
        "    # Create masks for the encoder input, decoder input and the padding\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n",
        "\n",
        "    # Perform a forward pass through the model\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp=encoder_input, tar=decoder_input,\n",
        "                                     enc_padding_mask=enc_padding_mask,\n",
        "                                     look_ahead_mask=combined_mask,\n",
        "                                     dec_padding_mask=dec_padding_mask,\n",
        "                                     training=True)\n",
        "\n",
        "        # Calculate the loss between the predicted output and the expected output\n",
        "        loss = loss_function(expected_output, predictions)\n",
        "\n",
        "    # Calculate gradients and update the model parameters\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    # Update the training loss and accuracy metrics\n",
        "    train_loss(loss)\n",
        "    train_accuracy(expected_output, predictions)\n",
        "\n",
        "    return loss, train_accuracy.result()"
      ],
      "metadata": {
        "id": "9CN0SjVaKuOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Re-initialize metrics at the start of each epoch\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "    current_batch_index = 0\n",
        "\n",
        "    # Iterate through the dataset in batches of batch_size\n",
        "    for i in range(int(len(spn_train) / batch_size)):\n",
        "        # Get the input and target batch\n",
        "        target_batch = tf.convert_to_tensor(np.array(spn_train[current_batch_index:current_batch_index + batch_size]), dtype=tf.int64)\n",
        "        input_batch = tf.convert_to_tensor(np.array(en_train[current_batch_index:current_batch_index + batch_size]), dtype=tf.int64)\n",
        "\n",
        "        current_batch_index = current_batch_index + batch_size\n",
        "\n",
        "        # Call the train_step function to train the model using the current batch\n",
        "        loss, accuracy = train_step(input_batch, target_batch)\n",
        "\n",
        "        # Update the metrics with the batch results\n",
        "        train_loss.update_state(loss)\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_batch, target_batch[:, :-1])\n",
        "        predictions, _ = transformer(inp=input_batch, tar=target_batch[:, :-1],\n",
        "                                     enc_padding_mask=enc_padding_mask,\n",
        "                                     look_ahead_mask=combined_mask,\n",
        "                                     dec_padding_mask=dec_padding_mask,\n",
        "                                     training=False) # Set training=False as we are evaluating this batch\n",
        "        train_accuracy.update_state(target_batch[:,1:], predictions)  # Assuming target_batch are the labels for classification\n",
        "\n",
        "    # Print the epoch loss and accuracy after iterating through the dataset\n",
        "    print(f'Epoch {epoch} - Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8de279-d308-4a29-9905-67c1302eaa54",
        "id": "7gLKdLDqCsy3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 1.1486, Accuracy: 0.8288\n",
            "Epoch 2 - Loss: 1.0882, Accuracy: 0.8353\n",
            "Epoch 3 - Loss: 1.0340, Accuracy: 0.8411\n",
            "Epoch 4 - Loss: 0.9901, Accuracy: 0.8468\n",
            "Epoch 5 - Loss: 0.9418, Accuracy: 0.8518\n",
            "Epoch 6 - Loss: 0.8885, Accuracy: 0.8585\n",
            "Epoch 7 - Loss: 0.8359, Accuracy: 0.8656\n",
            "Epoch 8 - Loss: 0.7947, Accuracy: 0.8713\n",
            "Epoch 9 - Loss: 0.7471, Accuracy: 0.8777\n",
            "Epoch 10 - Loss: 0.6921, Accuracy: 0.8855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Loop"
      ],
      "metadata": {
        "id": "ax4FjCvKI5tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation function\n",
        "def evaluate(transformer, spn_test, en_test, batch_size=64):\n",
        "    \"\"\"\n",
        "    Evaluates the Transformer model on the test set.\n",
        "\n",
        "    Args:\n",
        "        transformer (tf.keras.Model): The trained Transformer model.\n",
        "        spn_test (list): The target (Spanish) test set.\n",
        "        en_test (list): The input (English) test set.\n",
        "        batch_size (int): The batch size for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: The overall accuracy percentage on the test set.\n",
        "    \"\"\"\n",
        "    # Initialize the metric to compute accuracy\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "    current_batch_index = 0\n",
        "    num_batches = len(spn_test) // batch_size\n",
        "\n",
        "    # Iterate through the test dataset in batches\n",
        "    for i in range(num_batches):\n",
        "        # Get the input and target batch\n",
        "        target_batch = tf.convert_to_tensor(np.array(spn_test[current_batch_index:current_batch_index + batch_size]), dtype=tf.int64)\n",
        "        input_batch = tf.convert_to_tensor(np.array(en_test[current_batch_index:current_batch_index + batch_size]), dtype=tf.int64)\n",
        "\n",
        "        current_batch_index += batch_size\n",
        "\n",
        "        # Create masks for the encoder input, decoder input, and padding\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_batch, target_batch[:, :-1])\n",
        "\n",
        "        # Perform inference (no gradient updates) using the trained model\n",
        "        predictions, _ = transformer(inp=input_batch, tar=target_batch[:, :-1],\n",
        "                                     enc_padding_mask=enc_padding_mask,\n",
        "                                     look_ahead_mask=combined_mask,\n",
        "                                     dec_padding_mask=dec_padding_mask,\n",
        "                                     training=False)  # Set training=False during evaluation\n",
        "\n",
        "        # Update the accuracy metric\n",
        "        test_accuracy.update_state(target_batch[:, 1:], predictions)\n",
        "\n",
        "    # Return the final accuracy percentage\n",
        "    return test_accuracy.result().numpy() * 100  # Convert to percentage\n",
        "\n",
        "# After training, evaluate the model\n",
        "final_accuracy = evaluate(transformer, spn_test, en_test, batch_size)\n",
        "print(f\"Model's Accuracy : {final_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDw4bD22NeEg",
        "outputId": "6ee415c0-0193-473f-dc50-ab8a0d4a02bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Accuracy : 83.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tuning/ Pretrained Model**"
      ],
      "metadata": {
        "id": "V48HQ5Fk_OXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Downloading Libraries"
      ],
      "metadata": {
        "id": "f6yTvQiwL-Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece] sacrebleu -q"
      ],
      "metadata": {
        "id": "QuEc0cdi6iHz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:16:56.859007Z",
          "iopub.execute_input": "2024-12-23T19:16:56.859322Z",
          "iopub.status.idle": "2024-12-23T19:17:00.496606Z",
          "shell.execute_reply.started": "2024-12-23T19:16:56.859292Z",
          "shell.execute_reply": "2024-12-23T19:17:00.495589Z"
        },
        "outputId": "edfa8142-2d36-4078-f5a9-83e3f5f7f880"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from datasets import Dataset\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "7vD9FXhN6oHO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:17:10.325355Z",
          "iopub.execute_input": "2024-12-23T19:17:10.325685Z",
          "iopub.status.idle": "2024-12-23T19:17:22.237997Z",
          "shell.execute_reply.started": "2024-12-23T19:17:10.325651Z",
          "shell.execute_reply": "2024-12-23T19:17:22.237284Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPUS-MT (Open Translation) project by Helsinki-NLP for EN-ES Translation"
      ],
      "metadata": {
        "id": "EHwzLPAkKgkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading & Preprocessing Data"
      ],
      "metadata": {
        "id": "Mb65XbgLpY4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\""
      ],
      "metadata": {
        "id": "uvx4Vw3O60fy",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:17:22.239138Z",
          "iopub.execute_input": "2024-12-23T19:17:22.239815Z",
          "iopub.status.idle": "2024-12-23T19:17:22.243409Z",
          "shell.execute_reply.started": "2024-12-23T19:17:22.239779Z",
          "shell.execute_reply": "2024-12-23T19:17:22.242614Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/enesdf/data.csv')\n",
        "print(data.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgOmYgSk5kdh",
        "outputId": "c4319ee6-2b23-47c1-cb89-92c17f4d84b1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:15.678847Z",
          "iopub.execute_input": "2024-12-23T19:18:15.679201Z",
          "iopub.status.idle": "2024-12-23T19:18:15.987224Z",
          "shell.execute_reply.started": "2024-12-23T19:18:15.679160Z",
          "shell.execute_reply": "2024-12-23T19:18:15.986388Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  english  spanish\n0     Go.      Ve.\n1     Go.    Vete.\n2     Go.    Vaya.\n3     Go.  Váyase.\n4     Hi.    Hola.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=data[:7000]\n",
        "val_df=data[7000:8000]"
      ],
      "metadata": {
        "id": "bBiZdmmr9I6o",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:31:14.035140Z",
          "iopub.execute_input": "2024-12-23T19:31:14.035431Z",
          "iopub.status.idle": "2024-12-23T19:31:14.039484Z",
          "shell.execute_reply.started": "2024-12-23T19:31:14.035406Z",
          "shell.execute_reply": "2024-12-23T19:31:14.038529Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "K7TA4q4VFqKb",
        "outputId": "faf1a61e-ab78-41bf-a9c6-fe0dd0a07d96",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:15.993392Z",
          "iopub.execute_input": "2024-12-23T19:18:15.993635Z",
          "iopub.status.idle": "2024-12-23T19:18:16.010300Z",
          "shell.execute_reply.started": "2024-12-23T19:18:15.993614Z",
          "shell.execute_reply": "2024-12-23T19:18:16.009409Z"
        }
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  english  spanish\n0     Go.      Ve.\n1     Go.    Vete.\n2     Go.    Vaya.\n3     Go.  Váyase.\n4     Hi.    Hola.",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english</th>\n      <th>spanish</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Go.</td>\n      <td>Ve.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Go.</td>\n      <td>Vete.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Go.</td>\n      <td>Vaya.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Go.</td>\n      <td>Váyase.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hi.</td>\n      <td>Hola.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "efe5534a49ad42f4a9f3a27798a61749",
            "ccc6ab15fbd94402b7a7a0ef01d3ef6c",
            "a6321d5032394011a47284fb186bc1c2",
            "87a5793ab32240f893f56bb9b35c773b",
            "8e6dd875a6ac4a6fba0191cee7fcf3bc"
          ]
        },
        "id": "NaYzMrio9Ycj",
        "outputId": "49eed2fb-a9d8-4339-bcad-e47b40c4ebd9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:16.036684Z",
          "iopub.execute_input": "2024-12-23T19:18:16.036936Z",
          "iopub.status.idle": "2024-12-23T19:18:17.823476Z",
          "shell.execute_reply.started": "2024-12-23T19:18:16.036907Z",
          "shell.execute_reply": "2024-12-23T19:18:17.822462Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efe5534a49ad42f4a9f3a27798a61749"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccc6ab15fbd94402b7a7a0ef01d3ef6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6321d5032394011a47284fb186bc1c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87a5793ab32240f893f56bb9b35c773b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e6dd875a6ac4a6fba0191cee7fcf3bc"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "source_lang = \"english\"\n",
        "target_lang = \"spanish\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the inputs (source language)\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "cBiKAD0f9e-z",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:17.824465Z",
          "iopub.execute_input": "2024-12-23T19:18:17.824816Z",
          "iopub.status.idle": "2024-12-23T19:18:17.829489Z",
          "shell.execute_reply.started": "2024-12-23T19:18:17.824780Z",
          "shell.execute_reply": "2024-12-23T19:18:17.828745Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to Hugging Face Dataset\n",
        "train_set = Dataset.from_pandas(pd.DataFrame(train_set))\n",
        "val_set = Dataset.from_pandas(pd.DataFrame(val_set))\n",
        "\n",
        "#Apply the preprocessing function to the datasets\n",
        "tokenized_train = train_set.map(preprocess_function, batched=True)\n",
        "tokenized_val = val_set.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "fcd02b62312140f187a09691d8ea266b",
            "d3755a6c38f948eab891c9b84a334002",
            "bb195d48bbe54606971c9f3ec1fe1781",
            "83333325886d4369a9cb8ef4f2a6d697",
            "0622d07c13b84e789de809720ae32d36",
            "c31607210e114f10a0241b258374a701",
            "e8a10e67588841ff9aaa7c799c0c1a4a",
            "b406c34db04c42e6a0ac7657eee47367",
            "8245652573994a8ea3b95eadbe145769",
            "da92d9d09c3c4f1891b785dcf107effa",
            "b115595325214b66a19c2f6a5989bb76",
            "d100c18636d744e1b81bf8f925686996",
            "7ccf18734b9947738374f17df14e97f4",
            "9ae5cd5ac7df4a00ac655ba35d4d1928",
            "b8ffff8ee9544bf2b6dbae589a8ae96d",
            "cbae4edd87534a2cb0ca4ce0502ac8d0",
            "d2c6605bc1f74afe857b3e559df42d3a",
            "58bfdfe1a76e40d6ba67ea537471597f",
            "5874e5458682423f9c4e62d5752d0f75",
            "13a4597b58ea4bb0b1ef2a82b220d7a1",
            "1f913e6a7801439da976350cfed0afe6",
            "575d64a2dc494803b35adf63e2997d03",
            "2a9bf746126745fca9a87c0935082456",
            "71570a128c304beeaee0ef5622961853",
            "e6633ff03c0f4f84a265585aac43604a",
            "364b299598634aba88e663bad9f8d068",
            "23bf9e90a39249a093d55f9e5502ef90",
            "f3402efa75a44dcfb7fb8f70e6e8c9d7",
            "24873f9bc677465cacbbdbc305eb2862",
            "e0a4415ed503431ea9b9d2bc87ee5db5",
            "5a3d9ec845f04d20b45a9655e5a86df1",
            "6fe1fab79d7f4a228524fefb08822ba8",
            "c2c842f9da43404091aa59749601c84b",
            "0c80b77d766a42d091ae97883c6c9003",
            "c4bf057c74ab43efad55caa506bf4f2c",
            "53fef93c117c46c9bed01392ca96215e"
          ]
        },
        "id": "b7J8ig6v92bj",
        "outputId": "123a100a-0fb5-41bf-9576-cc64ee21ace1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:32:45.486303Z",
          "iopub.execute_input": "2024-12-23T19:32:45.486640Z",
          "iopub.status.idle": "2024-12-23T19:32:48.288358Z",
          "shell.execute_reply.started": "2024-12-23T19:32:45.486610Z",
          "shell.execute_reply": "2024-12-23T19:32:48.287706Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c80b77d766a42d091ae97883c6c9003"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4bf057c74ab43efad55caa506bf4f2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53fef93c117c46c9bed01392ca96215e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "GK76PX95pkbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e999c0f6448d45efa4444ad5240698cf",
            "5f5ba7656bc740cd92d307b7f6e5347b"
          ]
        },
        "id": "VqbZwxe3JTFO",
        "outputId": "c61d39f7-58fd-4605-dcdd-8616e2afc3ea",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:19.835404Z",
          "iopub.execute_input": "2024-12-23T19:18:19.835668Z",
          "iopub.status.idle": "2024-12-23T19:18:25.174102Z",
          "shell.execute_reply.started": "2024-12-23T19:18:19.835646Z",
          "shell.execute_reply": "2024-12-23T19:18:25.173204Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tf_model.h5:   0%|          | 0.00/313M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e999c0f6448d45efa4444ad5240698cf"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "All model checkpoint layers were used when initializing TFMarianMTModel.\n\nAll the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-es.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f5ba7656bc740cd92d307b7f6e5347b"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "GB_IEfmgJhC8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:25.175378Z",
          "iopub.execute_input": "2024-12-23T19:18:25.175659Z",
          "iopub.status.idle": "2024-12-23T19:18:25.179340Z",
          "shell.execute_reply.started": "2024-12-23T19:18:25.175636Z",
          "shell.execute_reply": "2024-12-23T19:18:25.178437Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "pbDxSEwGJk7Y",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:25.180250Z",
          "iopub.execute_input": "2024-12-23T19:18:25.180562Z",
          "iopub.status.idle": "2024-12-23T19:18:25.194184Z",
          "shell.execute_reply.started": "2024-12-23T19:18:25.180512Z",
          "shell.execute_reply": "2024-12-23T19:18:25.193363Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare TensorFlow dataset for training\n",
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_val,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "xllx4M94JrV1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:25.195030Z",
          "iopub.execute_input": "2024-12-23T19:18:25.195312Z",
          "iopub.status.idle": "2024-12-23T19:18:26.584499Z",
          "shell.execute_reply.started": "2024-12-23T19:18:25.195281Z",
          "shell.execute_reply": "2024-12-23T19:18:26.583609Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up optimizer and compile model\n",
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "yZWK6oNnJxIz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:26.585436Z",
          "iopub.execute_input": "2024-12-23T19:18:26.585696Z",
          "iopub.status.idle": "2024-12-23T19:18:26.599599Z",
          "shell.execute_reply.started": "2024-12-23T19:18:26.585673Z",
          "shell.execute_reply": "2024-12-23T19:18:26.598734Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bom5d_eLJyT7",
        "outputId": "683014c6-3315-4949-8abd-ca3f079cdee6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:18:26.600481Z",
          "iopub.execute_input": "2024-12-23T19:18:26.600787Z",
          "iopub.status.idle": "2024-12-23T19:28:21.417014Z",
          "shell.execute_reply.started": "2024-12-23T19:18:26.600751Z",
          "shell.execute_reply": "2024-12-23T19:28:21.416283Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n437/437 [==============================] - 89s 137ms/step - loss: 0.6708 - val_loss: 0.5426\nEpoch 2/10\n437/437 [==============================] - 56s 127ms/step - loss: 0.4972 - val_loss: 0.5493\nEpoch 3/10\n437/437 [==============================] - 56s 129ms/step - loss: 0.4112 - val_loss: 0.5666\nEpoch 4/10\n437/437 [==============================] - 57s 129ms/step - loss: 0.3533 - val_loss: 0.5828\nEpoch 5/10\n437/437 [==============================] - 56s 128ms/step - loss: 0.3115 - val_loss: 0.5995\nEpoch 6/10\n437/437 [==============================] - 56s 129ms/step - loss: 0.2854 - val_loss: 0.6233\nEpoch 7/10\n437/437 [==============================] - 56s 129ms/step - loss: 0.2628 - val_loss: 0.6295\nEpoch 8/10\n437/437 [==============================] - 56s 129ms/step - loss: 0.2476 - val_loss: 0.6502\nEpoch 9/10\n437/437 [==============================] - 56s 128ms/step - loss: 0.2351 - val_loss: 0.6702\nEpoch 10/10\n437/437 [==============================] - 56s 129ms/step - loss: 0.2236 - val_loss: 0.6777\n",
          "output_type": "stream"
        },
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tf_keras.src.callbacks.History at 0x7d1cb4f55600>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"tf_model/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcZFlK-ew0x3",
        "outputId": "1d6d95f4-cda6-402e-d29e-53c74689be23",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T19:28:21.417988Z",
          "iopub.execute_input": "2024-12-23T19:28:21.418240Z",
          "iopub.status.idle": "2024-12-23T19:28:22.489063Z",
          "shell.execute_reply.started": "2024-12-23T19:28:21.418218Z",
          "shell.execute_reply": "2024-12-23T19:28:22.488362Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "_wUDMmC2nwZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")\n",
        "\n",
        "# Function to translate the sentences\n",
        "def translate(text):\n",
        "    input_text  = text\n",
        "    tokenized = tokenizer([input_text], return_tensors='np')\n",
        "    out = model.generate(**tokenized, max_length=128)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10c8bc2-c57a-4028-c9fe-0e4892dceebe",
        "id": "R7XvkSpvPPxI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at tf_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"He's very intelligent.\"\n",
        "translate(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393ade0b-8258-43ac-8c3f-d61ffa42eadb",
        "id": "V58S4VKiPUH_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Él es muy inteligente.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WjKctk1Fzn-V",
        "QRh7tUtBU2oV",
        "FHfTgzIRQbwG",
        "FiifEW24GSAG",
        "QNSGFSOmZtwW",
        "5WgutrYfaNGk",
        "nT2iHaEpaOX9",
        "Klz7u5xR-8yS",
        "VSLgUalyFjZl",
        "wvrwB1O7NgUQ",
        "vF8LQCV-34Jd",
        "V48HQ5Fk_OXC",
        "f6yTvQiwL-Mm",
        "Mb65XbgLpY4U",
        "GK76PX95pkbD",
        "_wUDMmC2nwZc"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}